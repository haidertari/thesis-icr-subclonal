{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os,sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import helper\n",
    "import simulation\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "\n",
    "# check keras-like model summary using torchsummary\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "from ResNetUNet import ResNetUNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_path = ''\n",
    "image_path = os.path.join(base_path,'ValidationImages','images')\n",
    "mask_path = os.path.join(base_path,'ValidationImages','masks')\n",
    "image_tile_path = os.path.join(base_path,'ValidationTiles','images')\n",
    "mask_tile_path = os.path.join(base_path,'ValidationTiles','masks')\n",
    "\n",
    "imagepaths = glob.glob(image_path+'/*tif')\n",
    "print(len(imagepaths))\n",
    "\n",
    "for i in range(len(imagepaths)):\n",
    "    print(imagepaths[i])\n",
    "    image = cv2.imread(imagepaths[i], cv2.IMREAD_GRAYSCALE)\n",
    "    image_name = os.path.splitext(os.path.basename(imagepaths[i]))[0]\n",
    "    mask_name = os.path.join(mask_path, image_name+'.png')\n",
    "    mask = cv2.imread(mask_name, cv2.IMREAD_GRAYSCALE)\n",
    "    print(mask_path)\n",
    "    for x in range(0, image.shape[0], 112):\n",
    "        for y in range(0, image.shape[1], 112):\n",
    "            for a in [0, 90, 180, 270]:\n",
    "                M = cv2.getRotationMatrix2D((112, 112), a, 1)\n",
    "                \n",
    "                image_tile = cv2.warpAffine(image[x:x+224, y:y+224], M, (224, 224))\n",
    "                mask_tile = cv2.warpAffine(mask[x:x+224, y:y+224], M, (224, 224))\n",
    "                \n",
    "                cv2.imwrite(os.path.join(image_tile_path, image_name+'_'+str(x)+'_'+str(y)+'_'+str(a)+'.png'), image_tile)\n",
    "                cv2.imwrite(os.path.join(mask_tile_path, image_name+'_'+str(x)+'_'+str(y)+'_'+str(a)+'.png'), mask_tile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_path = os.path.join(base_path,'TrainingImages','images')\n",
    "mask_path = os.path.join(base_path,'TrainingImages','masks')\n",
    "image_tile_path = os.path.join(base_path,'TrainingTiles','images')\n",
    "mask_tile_path = os.path.join(base_path,'TrainingTiles','masks')\n",
    "\n",
    "imagepaths = glob.glob(image_path+'/*tif')\n",
    "print(len(imagepaths))\n",
    "\n",
    "for i in range(len(imagepaths)):\n",
    "    print(imagepaths[i])\n",
    "    image = cv2.imread(imagepaths[i], cv2.IMREAD_GRAYSCALE)\n",
    "    image_name = os.path.splitext(os.path.basename(imagepaths[i]))[0]\n",
    "    mask_name = os.path.join(mask_path, image_name+'.png')\n",
    "    mask = cv2.imread(mask_name, cv2.IMREAD_GRAYSCALE)\n",
    "    print(mask_path)\n",
    "    for x in range(0, image.shape[0], 112):\n",
    "        for y in range(0, image.shape[1], 112):\n",
    "            for a in [0, 90, 180, 270]:\n",
    "                M = cv2.getRotationMatrix2D((112, 112), a, 1)\n",
    "                \n",
    "                image_tile = cv2.warpAffine(image[x:x+224, y:y+224], M, (224, 224))\n",
    "                mask_tile = cv2.warpAffine(mask[x:x+224, y:y+224], M, (224, 224))\n",
    "                \n",
    "                cv2.imwrite(os.path.join(image_tile_path, image_name+'_'+str(x)+'_'+str(y)+'_'+str(a)+'.png'), image_tile)\n",
    "                cv2.imwrite(os.path.join(mask_tile_path, image_name+'_'+str(x)+'_'+str(y)+'_'+str(a)+'.png'), mask_tile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os,sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import helper\n",
    "import simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import glob\n",
    "\n",
    "class InvasionDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.image_paths = glob.glob(os.path.join(root_dir, 'images', '*.png'))\n",
    "        \n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = cv2.imread(self.image_paths[idx], cv2.IMREAD_COLOR)\n",
    "\n",
    "        mask_name = os.path.join(self.root_dir, 'masks', os.path.basename(self.image_paths[idx]))\n",
    "        \n",
    "        mask = cv2.imread(mask_name, cv2.IMREAD_GRAYSCALE)\n",
    "        mask = np.float32(mask)/255;\n",
    "        \n",
    "        if mask.ndim < 3:\n",
    "            mask = np.expand_dims(mask, axis=2)\n",
    "            \n",
    "        mask = mask.transpose([2, 0, 1])\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return [image, mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "\n",
    "# use same transform for train/val for this example\n",
    "trans = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # imagenet\n",
    "])\n",
    "\n",
    "train_set = InvasionDataset('/mnt/haider/NewTraining/TrainingTiles', transform = trans)\n",
    "val_set = InvasionDataset('/mnt/haider/NewTraining/ValidationTiles', transform = trans)\n",
    "\n",
    "image_datasets = {\n",
    "    'train': train_set, 'val': val_set\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "    'val': DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "}\n",
    "\n",
    "dataset_sizes = {\n",
    "    x: len(image_datasets[x]) for x in image_datasets.keys()\n",
    "}\n",
    "\n",
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torchvision.utils\n",
    "\n",
    "def reverse_transform(inp):\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    inp = (inp * 255).astype(np.uint8)\n",
    "    #SimDataset\n",
    "    return inp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "base_model = models.resnet18(pretrained=False)\n",
    "    \n",
    "list(base_model.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check keras-like model summary using torchsummary\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "base_model = base_model.to(device)\n",
    "\n",
    "summary(base_model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def convrelu(in_channels, out_channels, kernel, padding, dropout=0.7):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout(p=dropout)\n",
    "    )\n",
    "\n",
    "class ResNetUNet(nn.Module):\n",
    "\n",
    "    def __init__(self, n_class, n_channel=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.base_model = models.resnet18(pretrained=True)\n",
    "        \n",
    "        self.base_layers = list(base_model.children())                \n",
    "        \n",
    "        self.layer0 = nn.Sequential(*self.base_layers[:3]) # size=(N, 64, x.H/2, x.W/2)\n",
    "        self.layer0_1x1 = convrelu(64, 64, 1, 0)\n",
    "        self.layer1 = nn.Sequential(*self.base_layers[3:5]) # size=(N, 256, x.H/4, x.W/4)        \n",
    "        self.layer1_1x1 = convrelu(64, 256, 1, 0)       \n",
    "        self.layer2 = self.base_layers[5]  # size=(N, 512, x.H/8, x.W/8)        \n",
    "        self.layer2_1x1 = convrelu(128, 512, 1, 0)  \n",
    "        self.layer3 = self.base_layers[6]  # size=(N, 1024, x.H/16, x.W/16)        \n",
    "        self.layer3_1x1 = convrelu(256, 512, 1, 0)  \n",
    "        self.layer4 = self.base_layers[7]  # size=(N, 2048, x.H/32, x.W/32)\n",
    "        self.layer4_1x1 = convrelu(512, 1024, 1, 0)\n",
    "        \n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        \n",
    "        self.conv_up3 = convrelu(512 + 1024, 512, 3, 1)\n",
    "        self.conv_up2 = convrelu(512 + 512, 512, 3, 1)\n",
    "        self.conv_up1 = convrelu(256 + 512, 256, 3, 1)\n",
    "        self.conv_up0 = convrelu(64 + 256, 128, 3, 1)\n",
    "        \n",
    "        self.conv_original_size0 = convrelu(n_channel, 64, 3, 1)\n",
    "        self.conv_original_size1 = convrelu(64, 64, 3, 1)\n",
    "        self.conv_original_size2 = convrelu(64 + 128, 64, 3, 1)\n",
    "        \n",
    "        self.conv_last = nn.Conv2d(64, n_class, 1)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x_original = self.conv_original_size0(input)\n",
    "        x_original = self.conv_original_size1(x_original)\n",
    "        \n",
    "        layer0 = self.layer0(input)\n",
    "        layer1 = self.layer1(layer0)\n",
    "        layer2 = self.layer2(layer1)\n",
    "        layer3 = self.layer3(layer2)    \n",
    "        layer4 = self.layer4(layer3)\n",
    "\n",
    "        layer4 = self.layer4_1x1(layer4)\n",
    "        x = self.upsample(layer4)\n",
    "        layer3 = self.layer3_1x1(layer3)\n",
    "        x = torch.cat([x, layer3], dim=1)\n",
    "        x = self.conv_up3(x)\n",
    " \n",
    "        x = self.upsample(x)\n",
    "        layer2 = self.layer2_1x1(layer2)\n",
    "        x = torch.cat([x, layer2], dim=1)\n",
    "        x = self.conv_up2(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer1 = self.layer1_1x1(layer1)\n",
    "        x = torch.cat([x, layer1], dim=1)\n",
    "        x = self.conv_up1(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer0 = self.layer0_1x1(layer0)\n",
    "        x = torch.cat([x, layer0], dim=1)\n",
    "        x = self.conv_up0(x)\n",
    "        \n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x_original], dim=1)\n",
    "        x = self.conv_original_size2(x)        \n",
    "        \n",
    "        out = self.conv_last(x)        \n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check keras-like model summary using torchsummary\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ResNetUNet(1)\n",
    "model = model.to(device)\n",
    "\n",
    "summary(model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "from loss import dice_loss\n",
    "\n",
    "def calc_loss(pred, target, metrics, bce_weight=0.5, pos_weight=2):\n",
    "    #pred = F.sigmoid(pred)\n",
    "    weights = torch.Tensor((pred.shape[1], )).fill_(pos_weight).cuda()\n",
    "    bce = F.binary_cross_entropy_with_logits(pred, target, pos_weight=weights)\n",
    "    #bce = F.binary_cross_entropy(pred, target)\n",
    "        \n",
    "    pred = F.sigmoid(pred)\n",
    "    dice = dice_loss(pred, target)\n",
    "    \n",
    "    loss = bce * bce_weight + dice * (1 - bce_weight)\n",
    "    \n",
    "    metrics['bce'] += bce.data.cpu().numpy() * target.size(0)\n",
    "    metrics['dice'] += dice.data.cpu().numpy() * target.size(0)\n",
    "    metrics['loss'] += loss.data.cpu().numpy() * target.size(0)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def print_metrics(metrics, epoch_samples, phase):    \n",
    "    outputs = []\n",
    "    for k in metrics.keys():\n",
    "        outputs.append(\"{}: {:4f}\".format(k, metrics[k] / epoch_samples))\n",
    "        \n",
    "    print(\"{}: {}\".format(phase, \", \".join(outputs)))    \n",
    "\n",
    "def train_model(model, optimizer, scheduler, num_epochs=25):\n",
    "    global best_loss\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        since = time.time()\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    print(\"LR\", param_group['lr'])\n",
    "                    \n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            metrics = defaultdict(float)\n",
    "            epoch_samples = 0\n",
    "            iteration = 0\n",
    "            \n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)             \n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = calc_loss(outputs, labels, metrics, bce_weight=0.5)\n",
    "                    \n",
    "                    if iteration % 1 == 0:\n",
    "                        \n",
    "                        time_elapsed = time.time() - since\n",
    "                    \n",
    "                        print(\"{}: loss:{} ({:.0f}m {:.0f}s)           \".format(phase, loss, time_elapsed // 60, time_elapsed % 60), end=\"\\r\")\n",
    "                    \n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                epoch_samples += inputs.size(0)\n",
    "                iteration += 1\n",
    "\n",
    "            print_metrics(metrics, epoch_samples, phase)\n",
    "            epoch_loss = metrics['loss'] / epoch_samples\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                print(\"saving best model\")\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "            \n",
    "    print('Best val loss: {:4f}'.format(best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "#num_class = len(train_set.labels)\n",
    "num_class = 1\n",
    "\n",
    "best_loss = 1e10\n",
    "\n",
    "model = ResNetUNet(num_class).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# freeze backbone layers\n",
    "classifier_path = ''\n",
    "model = torch.load(classifier_path, map_location=lambda storage, loc: storage)\n",
    "model = model.cuda().eval()\n",
    "\n",
    "        \n",
    "for l in model.base_layers:\n",
    "    for param in l.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-6)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=30, gamma=0.1)        \n",
    "        \n",
    "model = train_model(model, optimizer_ft, exp_lr_scheduler, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#### prediction\n",
    "\n",
    "import math\n",
    "\n",
    "model.eval()   # Set model to evaluate mode\n",
    "\n",
    "#test_dataset = SimDataset(3, transform = trans)\n",
    "#test_loader = DataLoader(test_dataset, batch_size=3, shuffle=False, num_workers=0)\n",
    "        \n",
    "inputs, labels = next(iter(dataloaders['val']))\n",
    "inputs = inputs.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "pred = model(inputs)\n",
    "pred = F.sigmoid(pred)\n",
    "pred = pred.data.cpu().numpy()\n",
    "print(pred.shape)\n",
    "\n",
    "# Change channel-order and make 3 channels for matplot\n",
    "input_images_rgb = [reverse_transform(x) for x in inputs.cpu()]\n",
    "\n",
    "# Map each channel (i.e. class) to each color\n",
    "target_masks_rgb = [helper.masks_to_colorimg(x) for x in labels.cpu().numpy()]\n",
    "pred_rgb = [helper.masks_to_colorimg(x) for x in pred]\n",
    "#pred_mask = [(255*np.transpose(np.tile(x[1,:,:], (3,1,1)), (1, 2, 0))).astype('uint8') for x in pred]\n",
    "#pred_mask = [(255*np.transpose(x[1:4,:,:], (1, 2, 0))).astype('uint8') for x in pred]\n",
    "\n",
    "helper.plot_side_by_side([input_images_rgb, target_masks_rgb, pred_rgb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"\")\n",
    "torch.save(model, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from ResNetUNet import ResNetUNet\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "classifier_path = ''\n",
    "model = torch.load(classifier_path, map_location=lambda storage, loc: storage)\n",
    "model = model.cuda().eval()\n",
    "\n",
    "image = cv2.imread('', cv2.IMREAD_COLOR)\n",
    "tile = image[0:224, 0:224,:]\n",
    "print(tile.dtype)\n",
    "\n",
    "trans = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # imagenet\n",
    "])\n",
    "newtile = torch.tensor(np.zeros((1,3,224,224)), dtype=torch.float).cuda()\n",
    "newtile[0,:,:,:] = trans(tile)\n",
    "print(np.max(newtile.cpu().numpy()))\n",
    "pred = model(newtile)\n",
    "pred = F.sigmoid(pred)\n",
    "pred = np.squeeze(pred.data.cpu().numpy())\n",
    "print(np.max(pred*255))\n",
    "plt.imshow(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
